{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Monitoraggio della Reputazione Aziendale con Analisi del Sentiment\n",
    "\n",
    "Questo notebook implementa un flusso end-to-end minimo per:\n",
    "- Caricare un modello di sentiment da Hugging Face: `cardiffnlp/twitter-roberta-base-sentiment-latest`.\n",
    "- Eseguire inferenza su un dataset pubblico e su testi liberi.\n",
    "- Calcolare metriche semplici (accuracy, F1).\n",
    "- Fornire indicazioni per pipeline CI/CD e monitoraggio continuo.\n",
    "\n",
    "Repository GitHub del progetto: [company_reputation_monitoring](https://github.com/gab-25/company_reputation_monitoring)"
   ],
   "id": "be8caf12a69e375c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Setup ambiente\n",
    "Installa le dipendenze necessarie."
   ],
   "id": "31012ae23fcd204f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip -q install transformers torch datasets evaluate accelerate pandas numpy scikit-learn tqdm",
   "id": "be05a3e4749bd148"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Import e configurazione\n",
    "Carichiamo le librerie principali e impostiamo il dispositivo (CPU/GPU)."
   ],
   "id": "919dc0a67fa7bf58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:09:36.774611Z",
     "start_time": "2025-09-09T07:09:28.701586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "id": "59ef6df9b7e6ca2e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gab25/Projects/company_reputation_monitoring/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Caricamento modello Hugging Face\n",
    "Useremo il modello indicato nel README: `cardiffnlp/twitter-roberta-base-sentiment-latest`."
   ],
   "id": "d0e6991c205ae402"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:10:33.950366Z",
     "start_time": "2025-09-09T07:10:19.244398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_ID = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)\n",
    "model.to(device)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=0 if device=='cuda' else -1, truncation=True)\n",
    "pipe('I love this product!')"
   ],
   "id": "e9b131efb327bd8c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9848045110702515}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Dataset di esempio\n",
    "Per una demo rapida, utilizziamo `tweet_eval` (task sentiment) da Hugging Face Datasets. Questo dataset ha label: 0=negative, 1=neutral, 2=positive."
   ],
   "id": "231d5cfb078d7822"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:12:01.137804Z",
     "start_time": "2025-09-09T07:11:54.828427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds = load_dataset('tweet_eval', 'sentiment')\n",
    "ds"
   ],
   "id": "6b841052b0c5584c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 45615/45615 [00:00<00:00, 2206497.33 examples/s]\n",
      "Generating test split: 100%|██████████| 12284/12284 [00:00<00:00, 1940961.78 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<00:00, 747714.41 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Mappatura etichette del modello\n",
    "Il modello CardiffNLP etichetta in ordine [negative, neutral, positive]. Allineiamo le predizioni a 0/1/2 per confronto con il dataset."
   ],
   "id": "ec48809a34561da9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:12:22.978431Z",
     "start_time": "2025-09-09T07:12:22.975414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id2label = model.config.id2label\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "label2id"
   ],
   "id": "7c5cb71e4caac4f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0, 'neutral': 1, 'positive': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Valutazione rapida\n",
    "Calcoliamo accuracy ed F1 sul validation (o test) del dataset per una valutazione veloce."
   ],
   "id": "cf4c84dfb47c96ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:12:42.840309Z",
     "start_time": "2025-09-09T07:12:35.014676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def predict_labels(texts: List[str]) -> List[int]:\n",
    "    preds = pipe(texts, truncation=True)\n",
    "    # preds è una lista di dict con 'label' e 'score'\n",
    "    # Mappiamo 'LABEL_0'/'negative' etc a 0/1/2\n",
    "    out = []\n",
    "    for p in preds:\n",
    "        lab = p['label']\n",
    "        # Alcuni pipeline restituiscono 'LABEL_X', altri stringhe. Normalizziamo.\n",
    "        if lab in label2id:\n",
    "            out.append(label2id[lab])\n",
    "        else:\n",
    "            # gestisce 'LABEL_0' stile Auto\n",
    "            if lab.startswith('LABEL_'):\n",
    "                out.append(int(lab.split('_')[-1]))\n",
    "            else:\n",
    "                # fallback: negative->0, neutral->1, positive->2\n",
    "                lab_lower = lab.lower()\n",
    "                if 'neg' in lab_lower:\n",
    "                    out.append(0)\n",
    "                elif 'neu' in lab_lower:\n",
    "                    out.append(1)\n",
    "                else:\n",
    "                    out.append(2)\n",
    "    return out\n",
    "\n",
    "eval_split = ds.get('validation', ds['test']) if 'validation' in ds else ds['test']\n",
    "texts = eval_split['text'][:200]  # sottocampione per velocità\n",
    "y_true = eval_split['label'][:200]\n",
    "y_pred = predict_labels(texts)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "print({'accuracy': acc, 'f1_macro': f1_macro})\n",
    "print(classification_report(y_true, y_pred, digits=3))"
   ],
   "id": "7e0adb6ed835d0ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.76, 'f1_macro': 0.7543497371083578}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.650     0.812     0.722        32\n",
      "           1      0.727     0.744     0.736        86\n",
      "           2      0.861     0.756     0.805        82\n",
      "\n",
      "    accuracy                          0.760       200\n",
      "   macro avg      0.746     0.771     0.754       200\n",
      "weighted avg      0.770     0.760     0.762       200\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Inferenza su testi liberi\n",
    "Eseguiamo batch prediction su una lista di frasi esempio e visualizziamo le probabilità."
   ],
   "id": "a0bdc13fbe6b9b93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:12:50.629487Z",
     "start_time": "2025-09-09T07:12:50.515677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "samples = [\n",
    "    'Il prodotto è fantastico, super consigliato!',\n",
    "    'Servizio clienti lento e poco utile.',\n",
    "    'Esperienza nella media, niente di speciale.'\n",
    "]\n",
    "preds = pipe(samples, return_all_scores=True)\n",
    "for s, pr in zip(samples, preds):\n",
    "    scores = {p['label']: round(p['score'], 3) for p in pr}\n",
    "    print(s, '->', scores)"
   ],
   "id": "ec059ab6898e6b7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il prodotto è fantastico, super consigliato! -> {'negative': 0.006, 'neutral': 0.016, 'positive': 0.978}\n",
      "Servizio clienti lento e poco utile. -> {'negative': 0.025, 'neutral': 0.884, 'positive': 0.091}\n",
      "Esperienza nella media, niente di speciale. -> {'negative': 0.039, 'neutral': 0.843, 'positive': 0.117}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gab25/Projects/company_reputation_monitoring/.venv/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Salvataggio risultati\n",
    "Esempio di salvataggio delle predizioni su CSV, utile per audit o monitoraggio."
   ],
   "id": "c7728795bf2c8053"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "out_df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred\n",
    "})\n",
    "out_path = 'sentiment_eval_sample.csv'\n",
    "out_df.to_csv(out_path, index=False)\n",
    "out_path"
   ],
   "id": "27f2e5e727bb8314"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
